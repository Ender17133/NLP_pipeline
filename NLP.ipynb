{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2psbfEdfeAaj"
   },
   "source": [
    "This assignment will focus primarily on the design, creation, and deployment of `functions`. This assignment will require to create a series of functions together will be used as a crude NLP pipeline that cleans text data, structures the data for analysis, then calculates the `tf-idf` scores for a series of documents. The tf-idf metric is used to calculate the importance of a word to a document. Specifically, it measures the importance of a word in a document while discounting its value if it is used very frequently in all documents. For example the word \"the\" is probably used a lot in a given document, but it is also used a lot in most documents, so should not be considered important.\n",
    "\n",
    "The functions should be created for in `both R and Python`.\n",
    "\n",
    "### Background Vocabulary\n",
    "- **Document**: A single instance of text\n",
    "- **Corpus**: A collection of documents\n",
    "- **Tokenize**: Structure a string into a list of words. Each item is a single token\n",
    "- **TF-IDF Score**: A formula that tries to measure how important a given word is to a document. This helps in understanding what a document may be about.\n",
    "\n",
    "\n",
    "### **TF-IDF**\n",
    "#### Term Frequency: Calculated at the document level\n",
    "$$\n",
    "\\begin{align}\n",
    "TF = \\frac{Term\\ Frequency}{Total\\ Number\\ of\\ Words\\ in\\ Document}\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "#-- Term Frequency Example\n",
    "\"The dog ate the cat\"\n",
    "Output [0]: {\n",
    "  \"The\": 2/5,\n",
    "  \"Dog\": 1/5,\n",
    "  \"Ate\": 1/5,\n",
    "  \"Cat\": 1/5\n",
    "}\n",
    "```\n",
    "#### Inverse Document Frequency - Calculated at the corpus level\n",
    "$$\n",
    "\\begin{align}\n",
    "IDF = log(\\frac{Number\\ of\\ Documents\\ in\\ a\\ corpus}\n",
    "          {Number\\ of\\ documents\\ that\\ contain\\ the\\ term}\\ \\ \\ \\  )\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "#-- Inverse Document Frequency for 2 documents\n",
    "[\n",
    " \"The dog ate the cat\",\n",
    " \"The stars are bright\"\n",
    "]\n",
    "Output [0]: {\n",
    "  \"The\":   log(2 / 2),\n",
    "  \"Dog\":   log(2 / 1),\n",
    "  \"Ate\":   log(2 / 1),\n",
    "  \"Cat\":   log(2 / 1),\n",
    "  \"stars\": log(2 / 1),\n",
    "  \"are\":   log(2 / 1),\n",
    "  \"bright\":log(2 / 1),\n",
    "}\n",
    "```\n",
    "#### TF * IDF\n",
    "$$\n",
    "\\begin{align}\n",
    "TFIDF = TF * IDF\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "#-- TFIDF for Doc1 using corpus of Doc1 and Doc2\n",
    "[\n",
    " \"The dog ate the cat\",\n",
    " \"The stars are bright\"\n",
    "]\n",
    "Output [0]: TFIDF for Doc 1 = {\n",
    "  \"The\":   (2/5) * log(2 / 2),\n",
    "  \"Dog\":   (1/5) * log(2 / 1),\n",
    "  \"Ate\":   (1/5) * log(2 / 1),\n",
    "  \"Cat\":   (1/5) * log(2 / 1),\n",
    "  \"stars\": (0/5) * log(2 / 1),\n",
    "  \"are\":   (0/5) * log(2 / 1),\n",
    "  \"bright\":(0/5) * log(2 / 1)\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Learn more about TF-IDF and the specifics of the formula [here](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtsczOcIeLla"
   },
   "source": [
    "## 1. Create the function clean_text (12.5 pts)\n",
    "@clean_text(text: str)\n",
    "  - standardizes text by upper casing, removing non-alphanumeric characters (leave spaces), and removing extra spacing within the text\n",
    "  - Aguments: text: str representing a string of text data\n",
    "  - Return: text: str representing the clean string of text data\n",
    "  ```\n",
    "  Example\n",
    "  Input  --> \"The dog, ate 2 cats. \"\n",
    "  Output --> \"THE DOG ATE 2 CATS\"\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbVV9jQF26IO"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CXqVrFT2rAO",
    "outputId": "7a2b515b-3a9e-4ab9-85e5-f4c6b59a1b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS A TEST SENTENCE\n"
     ]
    }
   ],
   "source": [
    "#-- Hint. regular expressions can be helpful for string manipulation\n",
    "#------ The following functions may be useful\n",
    "\n",
    "#1 Libraries\n",
    "# re (regular expressions) module that is used for sub function\n",
    "import re\n",
    "'''\n",
    "  #-- removes non-alpha numeric characters, except white space\n",
    "  text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "\n",
    "  #-- Replace all white space with single space\n",
    "  text = \" \".join(text.split())\n",
    "'''\n",
    "#2 Code\n",
    "def clean_text(text:str):\n",
    "  # function removes all characters from the text that are non-alpha numeric\n",
    "  result = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "  # after that, using upper(), text casing is changed to upper casing\n",
    "  result = result.upper()\n",
    "   # as a last step, extra white spaces are eliminated with split() func. and then words are joined back together by white space with join()\n",
    "  result = ' '.join(result.split())\n",
    "  # function returns modified text as an output\n",
    "  return result\n",
    "\n",
    "\n",
    "#--- test function with example\n",
    "#3 Testing\n",
    "print(clean_text(\"This is a test    sentence!\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMwAeH698hIz"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NewoT7O27EW",
    "outputId": "da0f648f-8317-45ea-ae33-a4bf4f4da7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"THIS IS A TEST SENTENCE\"\n"
     ]
    }
   ],
   "source": [
    "#-- Hint. regular expressions can be helpful for string manipulation\n",
    "#------ The following functions may be useful\n",
    "\n",
    "#-- removes non-alpha numeric characters, except white space\n",
    "# text = gsub('[^A-Za-z0-9 ]+', '', text)\n",
    "\n",
    "#-- Replace all white space with single space\n",
    "# str_squish(text) using the stringr library\n",
    "\n",
    "#1 Libraries\n",
    "# library that has useful str_squish() function, which will help us to replace all white space with single space\n",
    "library(stringr)\n",
    "\n",
    "#2 Code\n",
    "clean_text = function(text) {\n",
    "    # gsub eliminates all characters that are non-alpha numeric\n",
    "    result_clean = gsub('[^A-Za-z0-9 ]+', '', text)\n",
    "    # toupper() function is used to make upper casing of all words in the text\n",
    "    result_clean = toupper(result_clean)\n",
    "    # str_squish() function is then used to replace all white space with single space\n",
    "    result_clean = str_squish(result_clean)\n",
    "    # clean_text() function returns modified text as an output\n",
    "    return(result_clean)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#--- test function with example\n",
    "print(clean_text(\"This is a test    sentence!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu_1D8cM0QG_"
   },
   "source": [
    "##2. Create the function tokenize (12.5 pts)\n",
    "@tokenize(text: str)\n",
    "  - Turns a string of text data into a list of words\n",
    "  - Arguments: text str representing a string of text data\n",
    "  - Returns: list of words\n",
    "  ```\n",
    "  Example\n",
    "  Input  --> \"THE DOG ATE 2 CATS\"\n",
    "  Output --> [\"THE\", \"DOG\", \"ATE\", \"2\", \"CATS\"]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCo7rBWl28C3"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7F6fQhS2rgz",
    "outputId": "e6b3c43d-9087-4b53-ad25-378e23c16c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'sentence!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Code\n",
    "def tokenize(text:str):\n",
    "   # function divides words in the text into the elements of the list by using split()\n",
    "  result_tokenize = text.split()\n",
    "   # function then returns text variable result_tokenize as an output\n",
    "  return result_tokenize\n",
    "\n",
    "\n",
    "\n",
    "#--- test function with example\n",
    "#2. Testing\n",
    "tokenize(\"This is a test    sentence!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvkKTlAP28fx"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JYhPISNM28on",
    "outputId": "b17859bc-9ef3-4f2b-a775-cceaa2b579c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'THIS'</li><li>'IS'</li><li>'A'</li><li>'TEST'</li><li>'SENTENCE'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'THIS'\n",
       "\\item 'IS'\n",
       "\\item 'A'\n",
       "\\item 'TEST'\n",
       "\\item 'SENTENCE'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'THIS'\n",
       "2. 'IS'\n",
       "3. 'A'\n",
       "4. 'TEST'\n",
       "5. 'SENTENCE'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"THIS\"     \"IS\"       \"A\"        \"TEST\"     \"SENTENCE\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 Code\n",
    "tokenize = function(text) {\n",
    "    # strsplit() function splits words by the space and makes a list\n",
    "    result_tokenize = strsplit(text, ' ')[[1]]\n",
    "    # function returns list of words\n",
    "    return(result_tokenize)\n",
    "}\n",
    "\n",
    "\n",
    "#2 Test\n",
    "#--- test function with example\n",
    "tokenize(\"THIS IS A TEST SENTENCE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr2wP1WN0QPm"
   },
   "source": [
    "## 3. Create the function clean_corpus (12.5 pts)\n",
    "\n",
    "@clean_corpus(list_of_text: list)\n",
    "  - Takes a list of strings and calls clean_text() and tokenize() functions to each string in a list\n",
    "  - Arguments: text_list list represents a list of text.\n",
    "  - Returns: list of lists with string tokens\n",
    "  ```\n",
    "Example\n",
    "  Input  --> [\"The dog, ate 2 cats. \",\n",
    "              \"The stars- are so     bright!\"]\n",
    "  Output --> [[\"THE\", \"DOG\", \"ATE\", \"2\", \"CATS\"],\n",
    "              [\"THE\",\"STARS\",\"ARE\",\"SO\",\"BRIGHT\"]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUQKLLPl29bc"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFf19nuI2sCX",
    "outputId": "15867aa1-bd73-4aca-cbb6-479bc8b04e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS', 'A', 'TEST', 'SENTENCE'], ['HERE', 'IS', 'A', '2ND', 'EXAMPLE']]\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "def clean_corpus(list_of_text:list):\n",
    "  # function creates a blank clean_corpus list\n",
    "  clean_corpus = []\n",
    "  # for loop that iterates over element in the clean_corpus list\n",
    "  for document in list_of_text:\n",
    "    # function uses previously created clean_text() function and cleans elements from non-alpha numeric characters\n",
    "    text = clean_text(document)\n",
    "    # function then uses another created tokenize() function and makes a tokes list from each element\n",
    "    tokens = tokenize(text)\n",
    "     # function appends tokens list to the clean_corpus list to create a lists of lists\n",
    "    clean_corpus.append(tokens)\n",
    "\n",
    "  # function returns clean_corpus list\n",
    "  return clean_corpus # function returns clean_corpus list\n",
    "\n",
    "\n",
    "#--- test function with example\n",
    "#2 Testing\n",
    "test_corp = [\n",
    "    \"This is a test    sentence!\",\n",
    "    \"here&^% is a 2nd example....\"\n",
    "]\n",
    "print(clean_corpus(test_corp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gito6d8m2978"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PMN4OGw2-E0",
    "outputId": "ec39547d-9181-4036-dfb7-04e5d0a883ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \"THIS\"     \"IS\"       \"A\"        \"TEST\"     \"SENTENCE\"\n",
      "\n",
      "[[2]]\n",
      "[1] \"HERE\"    \"IS\"      \"A\"       \"2ND\"     \"EXAMPLE\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "clean_corpus = function(list_of_text) {\n",
    "    # blank list called clean_corpus\n",
    "    clean_corpus = list()\n",
    "    # for loop that iterates over each element in the list\n",
    "    for (document in list_of_text) {\n",
    "        # clean_text() function cleans input from uneccessary white spaces and characters, assigns the result to variable text\n",
    "        text = clean_text(document)\n",
    "        # text is then tokenized using function tokenize()\n",
    "        tokens = tokenize(text)\n",
    "        # final result is then appended to clean_corpus blank list\n",
    "        clean_corpus = c(clean_corpus, list(tokens))\n",
    "    }\n",
    "    # function returns clean_corpus list as a result\n",
    "    return(clean_corpus)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#2 Test\n",
    "#--- test function with example\n",
    "doc1 <- \"This is a test    sentence!\"\n",
    "doc2 <- \"here&^% is a 2nd example....\"\n",
    "test_corp <- list(doc1, doc2)\n",
    "\n",
    "corp <- clean_corpus(test_corp)\n",
    "print(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJBKgJC80QUi"
   },
   "source": [
    "##4. Create the function term_frequency (12.5 pts)\n",
    "\n",
    "\n",
    "@term_frequency(list_of_text: list)\n",
    "  - Takes a list of list (string tokens) output from clean_corpus and computes a word count/total words in each document saved in a list of dictionaries. The dictionaries should have keys that relate to a token (e.g. word) and the value is the count. Note this should relate to the TF in TFIDF. Therefore it should be the count of word divided by the the total number of words in a document.\n",
    "  - Arguments: list_of_text str representing a list of strings. This should be the output of text_to_tokens.\n",
    "  - Returns: list of dictionaries featuring word counts\n",
    "  ```\n",
    "  Illustrative Examples - Values may not be correct\n",
    "  Input  --> [[\"THE\", \"DOG\", \"ATE\", \"2\", \"CATS\"],\n",
    "              [\"THE\",\"STARS\",\"ARE\",\"SO\",\"BRIGHT\"]]\n",
    "  Output --> [{\"THE\":0.2, \"DOG\":0.2, \"ATE\":0.2, \"2\":0.2, \"CATS\":0.2},\n",
    "              {\"THE\":0.2,\"STARS\":0.2,\"ARE\":0.2,\"SO\":0.2,\"BRIGHT\":0.2}]\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35uM9HCV2-vh"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1-r2xAO2snW",
    "outputId": "0d12c35b-a056-4bae-e122-2577a38a0ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'THIS': 0.16666666666666666, 'IS': 0.3333333333333333, 'A': 0.16666666666666666, 'TEST': 0.16666666666666666, 'SENTENCE': 0.16666666666666666}, {'HERE': 0.2, 'IS': 0.2, 'A': 0.2, '2ND': 0.2, 'EXAMPLE': 0.2}]\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "def term_frequency(list_of_text:list):\n",
    "  # function makes a blank list that will have 2 dictionaries inside later\n",
    "  dictionary_list = []\n",
    "  # outer for loop iterates over each list in the list of lists\n",
    "  for document1 in list_of_text:\n",
    "    # blank dictionary for elements in each list\n",
    "    elements_dictionary = {}\n",
    "     # function then further iterates over each element of the list in the inner for loop\n",
    "    for element in document1:\n",
    "       # total length variable that computes total number of words in the list\n",
    "        total_length = len(document1)\n",
    "         # word_count variable that compute number of instances of specific word in the list\n",
    "        word_count = document1.count(element)\n",
    "        # we calculate term frequency inside of the dictionary by dividing number of instances of word in dictionary by total number of words there\n",
    "        elements_dictionary[element] = (word_count / total_length)\n",
    "    # dictionary is then appended to dictionary_list\n",
    "    dictionary_list.append(elements_dictionary)\n",
    "  # dictionary list is returned in the end of the function\n",
    "  return dictionary_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2 Testing\n",
    "print(term_frequency([['THIS', 'IS', 'A', 'TEST', 'SENTENCE', 'IS'], ['HERE', 'IS', 'A', '2ND', 'EXAMPLE']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg93OM812_Td"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKkgZhlj2_dX",
    "outputId": "7c57f40f-9a55-4f03-f0ba-179ace0d1641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "     THIS        IS         A      TEST  SENTENCE \n",
      "0.1666667 0.3333333 0.1666667 0.1666667 0.1666667 \n",
      "\n",
      "[[2]]\n",
      "   HERE      IS       A     2ND EXAMPLE \n",
      "    0.2     0.2     0.2     0.2     0.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-- token %in% names(tf_dict) may be useful to check if word is in dictionary\n",
    "#1 Code\n",
    "term_frequency = function(list_of_text) {\n",
    "    # blank list called dictionary_list\n",
    "    dictionary_list = list()\n",
    "    # for loop that iterates over each document in the input list\n",
    "    for (document1 in list_of_text) {\n",
    "        # temporary dictionary called elements_dictionary is created inside of the outer for loop\n",
    "        elements_dictionary = list()\n",
    "        # inner for loop that iterates over each word in the document\n",
    "        for (element in document1) {\n",
    "            # function records length of the document and assigns it to the total_length variable\n",
    "            total_length = length(document1)\n",
    "            # function counts number of instances of word in the document\n",
    "            word_count = sum(document1 == element)\n",
    "            # function then calculates a term frequency of word inside of the elements_dictionary by formula word_count / total_length\n",
    "            elements_dictionary[[element]] = (word_count / total_length)\n",
    "\n",
    "        }\n",
    "        # after innter loop finishes, the temporary dictionary is appended to the dictionary_list\n",
    "        dictionary_list = c(dictionary_list, list(unlist(elements_dictionary)))\n",
    "    }\n",
    "    # function returns dictionary_list as a result\n",
    "    return(dictionary_list)\n",
    "}\n",
    "\n",
    "\n",
    "#2 Testing\n",
    "l1 <- c('THIS', 'IS', 'A', 'TEST', 'SENTENCE', 'IS')\n",
    "l2 = c('HERE', 'IS', 'A', '2ND', 'EXAMPLE')\n",
    "corp = list(l1, l2)\n",
    "tf <- term_frequency(corp)\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTcXteSX0QZO"
   },
   "source": [
    "##5. Create the function inverse_document_frequency (12.5 pts)\n",
    "\n",
    "@inverse_document_frequency(list_of_text: list)\n",
    "  - Takes a list of strings and computes a word count saved in a  dictionary. The dictionary should have keys that relate to a token (e.g. word) and the value should be the inverse document frequency of that word\n",
    "  - Arguments: list_of_text str representing a list of strings. This should be the output of text_to_tokens.\n",
    "  - Returns: dictionary featuring word counts\n",
    "  ```\n",
    "  Illustrative Examples - Values may not be correct\n",
    "  Input  --> [[\"THE\", \"DOG\", \"ATE\", \"2\", \"CATS\"],\n",
    "              [\"THE\",\"STARS\",\"ARE\",\"SO\",\"BRIGHT\"]]\n",
    "  Output --> {\"THE\":0, \"DOG\":0, \"ATE\":0, \"2\":0, \"CATS\":0,\n",
    "              \"STARS\":0,\"ARE\":0,\"SO\":0,\"BRIGHT\":0}\n",
    "  ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsjPK00f3Aj8"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BGdvLTZ2tkC",
    "outputId": "0233fc1a-68c1-4d80-d6d2-0e69a00d3d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TODAY', 0.4054651081081644)\n",
      "('RAIN', 1.0986122886681098)\n",
      "('IS', 1.0986122886681098)\n",
      "('GOING', 0.0)\n",
      "('IT', 1.0986122886681098)\n",
      "('TO', 0.4054651081081644)\n",
      "('NOT', 1.0986122886681098)\n",
      "('OUTSIDE', 1.0986122886681098)\n",
      "('I', 0.4054651081081644)\n",
      "('AM', 0.4054651081081644)\n",
      "('WATCH', 1.0986122886681098)\n",
      "('THE', 1.0986122886681098)\n",
      "('PREMIERE', 1.0986122886681098)\n",
      "('SEASON', 1.0986122886681098)\n"
     ]
    }
   ],
   "source": [
    "#1 Libraries\n",
    "# important math library to calculate IDF\n",
    "import math\n",
    "\n",
    "#2 Code\n",
    "def inverse_document_frequency(list_of_text:list):\n",
    "  # list_length variable that calculates the length of input list\n",
    "  list_length = len(list_of_text)\n",
    "  # blank dictionary for elements in each list\n",
    "  elements_dictionary2 = {}\n",
    "  # outer for loop that iterates over each list (document) in the input\n",
    "  for document2 in list_of_text:\n",
    "    # inner for loop that uses set() function on list in the outer for loop which makes list count duplicate words only once.\n",
    "    for element1 in set(document2):\n",
    "      # if element is in the dictionary, add 1 to the value of the element in the dictionary\n",
    "      if element1 in elements_dictionary2:\n",
    "        elements_dictionary2[element1] += 1\n",
    "        # in any other scenario assign 1 to the value of the word in the dictionary\n",
    "      else:\n",
    "        elements_dictionary2[element1] = 1\n",
    "\n",
    "  # another outer for loop that iterates over names as element3 and value of elements as value from dictionary\n",
    "  for element3, value in elements_dictionary2.items():\n",
    "      # for loop then calculates idf\n",
    "      elements_dictionary2[element3] = math.log(list_length / value)\n",
    "\n",
    "  # function returns a dictionary\n",
    "  return elements_dictionary2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3 Testing\n",
    "test_corpus = [\n",
    "    [\"IT\", \"IS\", \"GOING\", \"TO\", \"RAIN\", \"TODAY\"],\n",
    "    [\"TODAY\", \"I\", \"AM\", \"NOT\", \"GOING\", \"OUTSIDE\"],\n",
    "    [\"I\", \"AM\", \"GOING\", \"TO\", \"WATCH\", \"THE\", \"SEASON\", \"PREMIERE\"]\n",
    "]\n",
    "for i in inverse_document_frequency(test_corpus).items():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlpyP-VB3BJc"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RHB_dNU3BVb",
    "outputId": "bec681e2-21da-4d65-bce7-f6a47c280624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"IT 1.09861228866811\"\n",
      "[1] \"IS 1.09861228866811\"\n",
      "[1] \"GOING 0\"\n",
      "[1] \"TO 0.405465108108164\"\n",
      "[1] \"RAIN 1.09861228866811\"\n",
      "[1] \"TODAY 0.405465108108164\"\n",
      "[1] \"I 0.405465108108164\"\n",
      "[1] \"AM 0.405465108108164\"\n",
      "[1] \"NOT 1.09861228866811\"\n",
      "[1] \"OUTSIDE 1.09861228866811\"\n",
      "[1] \"WATCH 1.09861228866811\"\n",
      "[1] \"THE 1.09861228866811\"\n",
      "[1] \"SEASON 1.09861228866811\"\n",
      "[1] \"PREMIERE 1.09861228866811\"\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "inverse_document_frequency = function(list_of_text) {\n",
    "    # list_length variable that records the length of the input list\n",
    "    list_length = length(list_of_text)\n",
    "    # blank list that will work as a dictionary in R called elements_dictionary2\n",
    "    elements_dictionary2 = list()\n",
    "    # outer for loop that iterates over each document in the list\n",
    "    for (document2 in list_of_text) {\n",
    "        # inner for loop that iterates over each element in the document\n",
    "        # we need unique() function as we want to calculate number of document that contain the term without including the duplicates\n",
    "        for (element1 in unique(document2)) {\n",
    "            # if statement that adds +1 to the number of instances of element if the element is already in the elements_dictionary\n",
    "            if (element1 %in% names(elements_dictionary2)) {\n",
    "                elements_dictionary2[[element1]] = elements_dictionary2[[element1]] + 1\n",
    "            }\n",
    "            # else statement that assigns 1 to number of instances of element in any other scenario\n",
    "            else {\n",
    "                elements_dictionary2[[element1]] = 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # another outer for loop that calculates the value of the word in the elements_dictionary by the formula log(list_length / number of documents with this word)\n",
    "    for (word in names(elements_dictionary2)) {\n",
    "        elements_dictionary2[[word]] = unlist(log(list_length / elements_dictionary2[[word]]))\n",
    "    }\n",
    "    # at the end, function returns the dictionary as a result\n",
    "    return(elements_dictionary2)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#2 Testing\n",
    "\n",
    "doc1 = c(\"IT\", \"IS\", \"GOING\", \"TO\", \"RAIN\", \"TODAY\")\n",
    "doc2 = c(\"TODAY\", \"I\", \"AM\", \"NOT\", \"GOING\", \"OUTSIDE\")\n",
    "doc3 = c(\"I\", \"AM\", \"GOING\", \"TO\", \"WATCH\", \"THE\", \"SEASON\", \"PREMIERE\")\n",
    "test_corpus = list(doc1, doc2, doc3)\n",
    "\n",
    "idf = inverse_document_frequency(test_corpus)\n",
    "for (name in names(idf)){\n",
    " print(paste(name, as.character(idf[name])))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYJIU1bu-l24"
   },
   "source": [
    "## 6. Create the tfidf function (12.5 pts)\n",
    "@tfidf(list_of_text: list)\n",
    "  - Takes a list of strings and calls the term_frequency and inverse_document_frequency functions to compute the TFIDF.\n",
    "  - Arguments: list_of_text str representing a list of strings. This should be the output of text_to_tokens.\n",
    "  - Returns: dictionary featuring word counts\n",
    "  ```\n",
    "  Illustrative Examples - Values may not be correct\n",
    "  Input  --> [[\"THE\", \"DOG\", \"ATE\", \"2\", \"CATS\"],\n",
    "              [\"THE\",\"STARS\",\"ARE\",\"SO\",\"BRIGHT\"]]\n",
    "  Output --> [{\"THE\":0, \"DOG\":0, \"ATE\":0, \"2\":0, \"CATS\":0},\n",
    "              {\"THE\":0,\"STARS\":0,\"ARE\":0,\"SO\":0,\"BRIGHT\":0}]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heMxC1K6-l5d"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6wAKjOu-mdM",
    "outputId": "8b156ae1-b461-4d0d-cfaf-c118908b9055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': 0.1831020481113516, 'IS': 0.1831020481113516, 'GOING': 0.0, 'TO': 0.06757751801802739, 'RAIN': 0.1831020481113516, 'TODAY': 0.06757751801802739}\n",
      "{'TODAY': 0.06757751801802739, 'I': 0.06757751801802739, 'AM': 0.06757751801802739, 'NOT': 0.1831020481113516, 'GOING': 0.0, 'OUTSIDE': 0.1831020481113516}\n",
      "{'I': 0.05068313851352055, 'AM': 0.05068313851352055, 'GOING': 0.0, 'TO': 0.05068313851352055, 'WATCH': 0.13732653608351372, 'THE': 0.13732653608351372, 'SEASON': 0.13732653608351372, 'PREMIERE': 0.13732653608351372}\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "def tfidf(list_of_text:list):\n",
    "  # function applies term_frequency() function on the input list and assigns the result to the tf variable\n",
    "  tf = term_frequency(list_of_text)\n",
    "  # function applies inverse_document_frequency function on the input list and assigns the result to the idf variable\n",
    "  idf = inverse_document_frequency(list_of_text)\n",
    "  # function creates blank list called tfidf_list\n",
    "  tfidf_list = []\n",
    "\n",
    "  # outer loop that iterates over each dictionary in tf variable\n",
    "  for dictionary_tf in tf:\n",
    "    # temporary dictionary that is created in outer for loop\n",
    "    temp_dictionary = {}\n",
    "    # innter for loop that iterates over each element and value of the element in the dictionary_tf\n",
    "    for element_tf, value_tf in dictionary_tf.items():\n",
    "      # if the element is already in idf, calculate tfidf by multiplying the value of idf by value of tf\n",
    "      if element_tf in idf:\n",
    "        temp_dictionary[element_tf] =  value_tf * idf[element_tf]\n",
    "    # temporary dictionary is then appended to tfidf_list\n",
    "    tfidf_list.append(temp_dictionary)\n",
    "\n",
    "  # tfidf_list is returned as a result of this function\n",
    "  return tfidf_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2 Testing\n",
    "test_corpus = [\n",
    "    [\"IT\", \"IS\", \"GOING\", \"TO\", \"RAIN\", \"TODAY\"],\n",
    "    [\"TODAY\", \"I\", \"AM\", \"NOT\", \"GOING\", \"OUTSIDE\"],\n",
    "    [\"I\", \"AM\", \"GOING\", \"TO\", \"WATCH\", \"THE\", \"SEASON\", \"PREMIERE\"]\n",
    "]\n",
    "\n",
    "for i in tfidf(test_corpus):\n",
    "  print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrwTZZBz-mlO"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVGi2vUl-muG",
    "outputId": "95fc7bf5-f833-4306-c7d2-45e84cd838ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        IT         IS      GOING         TO       RAIN      TODAY \n",
      "0.18310205 0.18310205 0.00000000 0.06757752 0.18310205 0.06757752 \n",
      "     TODAY          I         AM        NOT      GOING    OUTSIDE \n",
      "0.06757752 0.06757752 0.06757752 0.18310205 0.00000000 0.18310205 \n",
      "         I         AM      GOING         TO      WATCH        THE     SEASON \n",
      "0.05068314 0.05068314 0.00000000 0.05068314 0.13732654 0.13732654 0.13732654 \n",
      "  PREMIERE \n",
      "0.13732654 \n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "tfidf = function(list_of_text) {\n",
    "    # tf variable that uses term_frequency() function on input list\n",
    "    tf = term_frequency(list_of_text)\n",
    "    # idf variable that uses inverse_document_frequency() function on input list\n",
    "    idf = inverse_document_frequency(list_of_text)\n",
    "    # blank list called tfidf_list\n",
    "    tfidf_list = list()\n",
    "\n",
    "    # outer for loop that iterates over each dictionary in the modified tf list\n",
    "    for (dictionary_tf in tf) {\n",
    "        # outer loop creates a blank temporary dictionary\n",
    "        temp_dictionary = list()\n",
    "        # innner loop that iterates over each element in the dictionary\n",
    "        for (word in names(dictionary_tf)) {\n",
    "            # if the word is present as an element in idf variable, function calculates the value of this word in the temporary dictionary by using formula tf value * idf value\n",
    "            if (word %in% names(idf)) {\n",
    "                temp_dictionary[[word]] = dictionary_tf[[word]] * idf[[word]]\n",
    "            }\n",
    "          }\n",
    "        # temporary dictionary is then appended to our tfidf_list\n",
    "        tfidf_list[[length(tfidf_list) + 1]] = unlist(temp_dictionary)\n",
    "\n",
    "    }\n",
    "    # function returns tfidf list as a result\n",
    "    return(tfidf_list)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2 Testing\n",
    "\n",
    "doc1 = c(\"IT\", \"IS\", \"GOING\", \"TO\", \"RAIN\", \"TODAY\")\n",
    "doc2 = c(\"TODAY\", \"I\", \"AM\", \"NOT\", \"GOING\", \"OUTSIDE\")\n",
    "doc3 = c(\"I\", \"AM\", \"GOING\", \"TO\", \"WATCH\", \"THE\", \"SEASON\", \"PREMIERE\")\n",
    "test_corpus = list(doc1, doc2, doc3)\n",
    "\n",
    "for (i in tfidf(test_corpus)){\n",
    "  print(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejpU0lzz0qYv"
   },
   "source": [
    "## 7. Create the function tfidf_pipeline (12.5 pts)\n",
    "@TFIDF_Pipeline(list_of_text: list)\n",
    "  - Takes a corpus of text in the form of a list of strings, and runs the above functions to clean, tokenize, and calculate TFIDF for each word and each document\n",
    "  - Arguments: list_of_text str representing a list of strings.\n",
    "  - Returns: list of dictionaries featuring TFIDF scores for each word in each document. Each item in the list should represent the scores for that document and each key:value pair in the dictionaries should represent word and TFIDF score\n",
    "  ```\n",
    "  Illustrative Examples - Values may not be correct\n",
    "  Input  --> [\"The dog, ate 2 cats. \",\n",
    "              \"The stars- are so     bright!\"]\n",
    "  Output --> [{\"THE\":0, \"DOG\":0, \"ATE\":0, \"2\":0, \"CATS\":0},\n",
    "              {\"THE\":0,\"STARS\":0,\"ARE\":0,\"SO\":0,\"BRIGHT\":0}]\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYClnV6L3CEj"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2mbRudf2vZP",
    "outputId": "dc3b7175-9154-4ec3-e878-281c9ba02cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'THE': 0.0, 'DOG': 0.13862943611198905, 'ATE': 0.13862943611198905, '2': 0.13862943611198905, 'CATS': 0.13862943611198905}\n",
      "{'THE': 0.0, 'STARS': 0.13862943611198905, 'ARE': 0.13862943611198905, 'SO': 0.13862943611198905, 'BRIGHT': 0.13862943611198905}\n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "#final tfidf_pipeline function that should be used\n",
    "def tfidf_pipeline(list_of_text:list):\n",
    "  # function applies clean_corpus() function on the input list and assigns the result to the final_text variable\n",
    "  final_text = clean_corpus(list_of_text)\n",
    "  # function applies tfidf() function on the input list and assigns the result to the tfidf_list variable\n",
    "  tfidf_list = tfidf(final_text)\n",
    "\n",
    "  # function returns tfidf_list as a result\n",
    "  return tfidf_list\n",
    "\n",
    "text = [\"The dog, ate 2 cats. \",\n",
    "          \"The stars- are so     bright!\"]\n",
    "\n",
    "for i in tfidf_pipeline(text):\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tak7kVea3Clp"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HVuxiJY3CuO",
    "outputId": "bf20186b-897f-458b-be2b-02ee50e1423b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      THE       DOG       ATE         2      CATS \n",
      "0.0000000 0.1386294 0.1386294 0.1386294 0.1386294 \n",
      "      THE     STARS       ARE        SO    BRIGHT \n",
      "0.0000000 0.1386294 0.1386294 0.1386294 0.1386294 \n"
     ]
    }
   ],
   "source": [
    "#1 Code\n",
    "tfidf_pipeline = function(list_of_text) {\n",
    "    # pipeline uses clean_corpus() function on input list and assigns it to a final_text variable\n",
    "    final_text = clean_corpus(list_of_text)\n",
    "    # pipeline then applies tfidf() function on final_text variable and assigns the result to a tfidf_list variable\n",
    "    tfidf_list = tfidf(final_text)\n",
    "\n",
    "    # function returns tfidf_list as a result\n",
    "    return(tfidf_list)\n",
    "}\n",
    "\n",
    "#2 Testing\n",
    "text = c(\"The dog, ate 2 cats. \",\n",
    "          \"The stars- are so     bright!\")\n",
    "\n",
    "for (i in tfidf_pipeline(text)) {\n",
    "    print(i)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsC0aePA0qbU"
   },
   "source": [
    "##8. Run the pipeline on the following corpus (12.5 pts)\n",
    "- What are some of the words with the highest tfidf in each document? You do not need to program this, you can merely look at the results. Note that because we have a small number of documents, many words will have the highest scores. Explain why some of these have the highest scores.\n",
    "\n",
    "Answer:\n",
    "\n",
    "In document 1, Lane Thomas, Wednesday, Jon Lester have a high value of idf. It is because these are unique names and are not present in other documents. Jon Lester and Lane Thomas - names of people. Wednesday - a name of the day in the week\n",
    "\n",
    "In document 2, U.S, WSJ, Dollar are some of the words that have a high value of tfidf. Again, it is because these words are not present in other documents and they are important for the context of the text.\n",
    "\n",
    "In document 3, Vivek Ramaswamy, splash, are the words that have a high value of tfidf\n",
    "\n",
    "In document 4, presidential, primary, debate are some of the words that have a high value of tfidf\n",
    "\n",
    "In document 5, national, polls, states are some of the words that have a high value of tfidf\n",
    "\n",
    "In document 6, Tulane, group, five are some of the words that have a high value of tfidf\n",
    "\n",
    "In document 7, San Francisco, Friday are some of the words that have a high value of tfidf\n",
    "\n",
    "In document 8, Pelosi, lieutenants, Steny Hoyer, Jim Clyburn are some of the words that have high value of tfidf\n",
    "\n",
    "In document 9, Emerita Pelosi, talented, transformational, lifetime are some of the words that have high value of tfidf\n",
    "\n",
    "The key idea is that words that are more unique and important to the context of the text have higher number of tfidf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GdoNvL3JndEX"
   },
   "outputs": [],
   "source": [
    "#1 Text (variable) that is going to be used\n",
    "corpus = [\n",
    "    \"Right fielder Lane Thomas, 28 on Wednesday and playing at an all-star level, was acquired for now-retired Jon Lester. \",\n",
    "    \"Propelled by signs that the U.S. economy is motoring while the rest of the world flags, the WSJ Dollar Index has appreciated more than 2% over the past month.\",\n",
    "    \"At last week’s Republican debate, businessman Vivek Ramaswamy arguably made the biggest splash of any candidate.\",\n",
    "    \"With the first Republican primary debate in the books, presidential debate season is officially in full swing.\",\n",
    "    \"Candidates will need to hit at least 3 percent in two national polls, or 3 percent in one national poll and 3 percent in two polls conducted from separate early nominating states (Iowa, New Hampshire, South Carolina and Nevada), in order to qualify by the Sept. 25 deadline.\",\n",
    "    'Is Tulane once again the best of the Group of Five?',\n",
    "    \"The San Francisco Democrat and first female speaker of the House told volunteers on Friday that she would seek reelection in 2024.\",\n",
    "    \"Pelosi’s top lieutenants, Reps. Steny Hoyer (D-Md.) and Jim Clyburn (D-S.C.), similarly stepped down from their top leadership roles.\",\n",
    "    \"Speaker Emerita Pelosi is one of the most talented and transformational leaders of our lifetime.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sguk-dlnde4"
   },
   "outputs": [],
   "source": [
    "#1 Text (variable) that is going to be used\n",
    "doc1 <- c(\"Right fielder Lane Thomas, 28 on Wednesday and playing at an all-star level, was acquired for now-retired Jon Lester. \")\n",
    "doc2 <- c(\"Propelled by signs that the U.S. economy is motoring while the rest of the world flags, the WSJ Dollar Index has appreciated more than 2% over the past month.\")\n",
    "doc3 <- c(\"At last week’s Republican debate, businessman Vivek Ramaswamy arguably made the biggest splash of any candidate.\")\n",
    "doc4 <- c(\"With the first Republican primary debate in the books, presidential debate season is officially in full swing.\")\n",
    "doc5 <- c(\"Candidates will need to hit at least 3 percent in two national polls, or 3 percent in one national poll and 3 percent in two polls conducted from separate early nominating states (Iowa, New Hampshire, South Carolina and Nevada), in order to qualify by the Sept. 25 deadline.\")\n",
    "doc6 <- c('Is Tulane once again the best of the Group of Five?')\n",
    "doc7 <- c(\"The San Francisco Democrat and first female speaker of the House told volunteers on Friday that she would seek reelection in 2024.\")\n",
    "doc8 <- c(\"Pelosi’s top lieutenants, Reps. Steny Hoyer (D-Md.) and Jim Clyburn (D-S.C.), similarly stepped down from their top leadership roles.\")\n",
    "doc9 <- c(\"Speaker Emerita Pelosi is one of the most talented and transformational leaders of our lifetime.\")\n",
    "corpus <- c(doc1, doc2, doc2, doc4, doc5, doc6, doc7, doc8, doc9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0sr-teC3Nq-"
   },
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Seni9sMreKXm",
    "outputId": "87b2f0cc-381b-4f39-ae73-31a82a43b002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WEDNESDAY', 0.11564339880716945), ('WAS', 0.11564339880716945), ('THOMAS', 0.11564339880716945), ('RIGHT', 0.11564339880716945), ('PLAYING', 0.11564339880716945), ('ON', 0.07916196825138284), ('NOWRETIRED', 0.11564339880716945), ('LEVEL', 0.11564339880716945), ('LESTER', 0.11564339880716945), ('LANE', 0.11564339880716945), ('JON', 0.11564339880716945), ('FOR', 0.11564339880716945), ('FIELDER', 0.11564339880716945), ('AT', 0.057821699403584725), ('AND', 0.030936140258006263), ('AN', 0.11564339880716945), ('ALLSTAR', 0.11564339880716945), ('ACQUIRED', 0.11564339880716945), ('28', 0.11564339880716945)]\n",
      "[('WSJ', 0.07576636473573171), ('WORLD', 0.07576636473573171), ('WHILE', 0.07576636473573171), ('US', 0.07576636473573171), ('THE', 0.043330073841535546), ('THAT', 0.05186473781987152), ('THAN', 0.07576636473573171), ('SIGNS', 0.07576636473573171), ('REST', 0.07576636473573171), ('PROPELLED', 0.07576636473573171), ('PAST', 0.07576636473573171), ('OVER', 0.07576636473573171), ('OF', 0.020268505686279966), ('MOTORING', 0.07576636473573171), ('MORE', 0.07576636473573171), ('MONTH', 0.07576636473573171), ('IS', 0.027963110904011337), ('INDEX', 0.07576636473573171), ('HAS', 0.07576636473573171), ('FLAGS', 0.07576636473573171), ('ECONOMY', 0.07576636473573171), ('DOLLAR', 0.07576636473573171), ('BY', 0.05186473781987152), ('APPRECIATED', 0.07576636473573171), ('2', 0.07576636473573171)]\n",
      "[('WEEKS', 0.13732653608351372), ('VIVEK', 0.13732653608351372), ('THE', 0.015707151767556635), ('SPLASH', 0.13732653608351372), ('REPUBLICAN', 0.09400483729851714), ('RAMASWAMY', 0.13732653608351372), ('OF', 0.03673666655638244), ('MADE', 0.13732653608351372), ('LAST', 0.13732653608351372), ('DEBATE', 0.09400483729851714), ('CANDIDATE', 0.13732653608351372), ('BUSINESSMAN', 0.13732653608351372), ('BIGGEST', 0.13732653608351372), ('AT', 0.06866326804175686), ('ARGUABLY', 0.13732653608351372), ('ANY', 0.13732653608351372)]\n",
      "[('WITH', 0.12924850454918937), ('THE', 0.02956640332716543), ('SWING', 0.12924850454918937), ('SEASON', 0.12924850454918937), ('REPUBLICAN', 0.08847514098683966), ('PRIMARY', 0.12924850454918937), ('PRESIDENTIAL', 0.12924850454918937), ('OFFICIALLY', 0.12924850454918937), ('IS', 0.04770177742448993), ('IN', 0.12924850454918937), ('FULL', 0.12924850454918937), ('FIRST', 0.08847514098683966), ('DEBATE', 0.17695028197367932), ('BOOKS', 0.12924850454918937)]\n",
      "[('WILL', 0.0457755120278379), ('TWO', 0.0915510240556758), ('TO', 0.0915510240556758), ('THE', 0.005235717255852212), ('STATES', 0.0457755120278379), ('SOUTH', 0.0457755120278379), ('SEPT', 0.0457755120278379), ('SEPARATE', 0.0457755120278379), ('QUALIFY', 0.0457755120278379), ('POLLS', 0.0915510240556758), ('POLL', 0.0457755120278379), ('PERCENT', 0.13732653608351372), ('ORDER', 0.0457755120278379), ('OR', 0.0457755120278379), ('ONE', 0.03133494576617238), ('NOMINATING', 0.0457755120278379), ('NEW', 0.0457755120278379), ('NEVADA', 0.0457755120278379), ('NEED', 0.0457755120278379), ('NATIONAL', 0.0915510240556758), ('LEAST', 0.0457755120278379), ('IOWA', 0.0457755120278379), ('IN', 0.0915510240556758), ('HIT', 0.0457755120278379), ('HAMPSHIRE', 0.0457755120278379), ('FROM', 0.03133494576617238), ('EARLY', 0.0457755120278379), ('DEADLINE', 0.0457755120278379), ('CONDUCTED', 0.0457755120278379), ('CAROLINA', 0.0457755120278379), ('CANDIDATES', 0.0457755120278379), ('BY', 0.03133494576617238), ('AT', 0.02288775601391895), ('AND', 0.024491111037588293), ('3', 0.13732653608351372), ('25', 0.0457755120278379)]\n",
      "[('TULANE', 0.19974768884874725), ('THE', 0.04569353241471021), ('ONCE', 0.19974768884874725), ('OF', 0.1068703027094762), ('IS', 0.07372092874693899), ('GROUP', 0.19974768884874725), ('FIVE', 0.19974768884874725), ('BEST', 0.19974768884874725), ('AGAIN', 0.19974768884874725)]\n",
      "[('WOULD', 0.09987384442437362), ('VOLUNTEERS', 0.09987384442437362), ('TOLD', 0.09987384442437362), ('THE', 0.022846766207355106), ('THAT', 0.06836715439892156), ('SPEAKER', 0.06836715439892156), ('SHE', 0.09987384442437362), ('SEEK', 0.09987384442437362), ('SAN', 0.09987384442437362), ('REELECTION', 0.09987384442437362), ('ON', 0.06836715439892156), ('OF', 0.02671757567736905), ('IN', 0.04993692221218681), ('HOUSE', 0.09987384442437362), ('FRIDAY', 0.09987384442437362), ('FRANCISCO', 0.09987384442437362), ('FIRST', 0.06836715439892156), ('FEMALE', 0.09987384442437362), ('DEMOCRAT', 0.09987384442437362), ('AND', 0.02671757567736905), ('2024', 0.09987384442437362)]\n",
      "[('TOP', 0.2312867976143389), ('THEIR', 0.11564339880716945), ('STEPPED', 0.11564339880716945), ('STENY', 0.11564339880716945), ('SIMILARLY', 0.11564339880716945), ('ROLES', 0.11564339880716945), ('REPS', 0.11564339880716945), ('PELOSIS', 0.11564339880716945), ('LIEUTENANTS', 0.11564339880716945), ('LEADERSHIP', 0.11564339880716945), ('JIM', 0.11564339880716945), ('HOYER', 0.11564339880716945), ('FROM', 0.07916196825138284), ('DSC', 0.11564339880716945), ('DOWN', 0.11564339880716945), ('DMD', 0.11564339880716945), ('CLYBURN', 0.11564339880716945), ('AND', 0.030936140258006263)]\n",
      "[('TRANSFORMATIONAL', 0.1464816384890813), ('THE', 0.016754295218727077), ('TALENTED', 0.1464816384890813), ('SPEAKER', 0.1002718264517516), ('PELOSI', 0.1464816384890813), ('OUR', 0.1464816384890813), ('ONE', 0.1002718264517516), ('OF', 0.07837155532028255), ('MOST', 0.1464816384890813), ('LIFETIME', 0.1464816384890813), ('LEADERS', 0.1464816384890813), ('IS', 0.05406201441442192), ('EMERITA', 0.1464816384890813), ('AND', 0.03918577766014127)]\n"
     ]
    }
   ],
   "source": [
    "#2 Testing\n",
    "for i in tfidf_pipeline(corpus):\n",
    "  print(sorted(i.items(), reverse =  True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fks0iu_imUIb"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL8yIOaXmUK-"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNaoo8WO3PBe"
   },
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNnuE0UM3PKE",
    "outputId": "742bfe8c-fda2-4b47-870c-6ce0dcd3e3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RIGHT    FIELDER       LANE     THOMAS         28         ON  WEDNESDAY \n",
      "0.11564340 0.11564340 0.11564340 0.11564340 0.11564340 0.07916197 0.11564340 \n",
      "       AND    PLAYING         AT         AN    ALLSTAR      LEVEL        WAS \n",
      "0.03093614 0.11564340 0.07916197 0.11564340 0.11564340 0.11564340 0.11564340 \n",
      "  ACQUIRED        FOR NOWRETIRED        JON     LESTER \n",
      "0.11564340 0.11564340 0.11564340 0.11564340 0.11564340 \n",
      "  PROPELLED          BY       SIGNS        THAT         THE          US \n",
      " 0.05186474  0.03788318  0.05186474  0.03788318  0.04333007  0.05186474 \n",
      "    ECONOMY          IS    MOTORING       WHILE        REST          OF \n",
      " 0.05186474  0.02026851  0.05186474  0.05186474  0.05186474  0.02026851 \n",
      "      WORLD       FLAGS         WSJ      DOLLAR       INDEX         HAS \n",
      " 0.05186474  0.05186474  0.05186474  0.05186474  0.05186474  0.05186474 \n",
      "APPRECIATED        MORE        THAN           2        OVER        PAST \n",
      " 0.05186474  0.05186474  0.05186474  0.05186474  0.05186474  0.05186474 \n",
      "      MONTH \n",
      " 0.05186474 \n",
      "  PROPELLED          BY       SIGNS        THAT         THE          US \n",
      " 0.05186474  0.03788318  0.05186474  0.03788318  0.04333007  0.05186474 \n",
      "    ECONOMY          IS    MOTORING       WHILE        REST          OF \n",
      " 0.05186474  0.02026851  0.05186474  0.05186474  0.05186474  0.02026851 \n",
      "      WORLD       FLAGS         WSJ      DOLLAR       INDEX         HAS \n",
      " 0.05186474  0.05186474  0.05186474  0.05186474  0.05186474  0.05186474 \n",
      "APPRECIATED        MORE        THAN           2        OVER        PAST \n",
      " 0.05186474  0.05186474  0.05186474  0.05186474  0.05186474  0.05186474 \n",
      "      MONTH \n",
      " 0.05186474 \n",
      "        WITH          THE        FIRST   REPUBLICAN      PRIMARY       DEBATE \n",
      "  0.12924850   0.02956640   0.08847514   0.12924850   0.12924850   0.25849701 \n",
      "          IN        BOOKS PRESIDENTIAL       SEASON           IS   OFFICIALLY \n",
      "  0.12924850   0.12924850   0.12924850   0.12924850   0.03457569   0.12924850 \n",
      "        FULL        SWING \n",
      "  0.12924850   0.12924850 \n",
      " CANDIDATES        WILL        NEED          TO         HIT          AT \n",
      "0.045775512 0.045775512 0.045775512 0.091551024 0.045775512 0.031334946 \n",
      "      LEAST           3     PERCENT          IN         TWO    NATIONAL \n",
      "0.045775512 0.137326536 0.137326536 0.091551024 0.091551024 0.091551024 \n",
      "      POLLS          OR         ONE        POLL         AND   CONDUCTED \n",
      "0.091551024 0.045775512 0.031334946 0.045775512 0.024491111 0.045775512 \n",
      "       FROM    SEPARATE       EARLY  NOMINATING      STATES        IOWA \n",
      "0.031334946 0.045775512 0.045775512 0.045775512 0.045775512 0.045775512 \n",
      "        NEW   HAMPSHIRE       SOUTH    CAROLINA      NEVADA       ORDER \n",
      "0.045775512 0.045775512 0.045775512 0.045775512 0.045775512 0.045775512 \n",
      "    QUALIFY          BY         THE        SEPT          25    DEADLINE \n",
      "0.045775512 0.022887756 0.005235717 0.045775512 0.045775512 0.045775512 \n",
      "        IS     TULANE       ONCE      AGAIN        THE       BEST         OF \n",
      "0.05343515 0.19974769 0.19974769 0.19974769 0.04569353 0.19974769 0.10687030 \n",
      "     GROUP       FIVE \n",
      "0.19974769 0.19974769 \n",
      "       THE        SAN  FRANCISCO   DEMOCRAT        AND      FIRST     FEMALE \n",
      "0.02284677 0.09987384 0.09987384 0.09987384 0.02671758 0.06836715 0.09987384 \n",
      "   SPEAKER         OF      HOUSE       TOLD VOLUNTEERS         ON     FRIDAY \n",
      "0.06836715 0.02671758 0.09987384 0.09987384 0.09987384 0.06836715 0.09987384 \n",
      "      THAT        SHE      WOULD       SEEK REELECTION         IN       2024 \n",
      "0.04993692 0.09987384 0.09987384 0.09987384 0.09987384 0.04993692 0.09987384 \n",
      "    PELOSIS         TOP LIEUTENANTS        REPS       STENY       HOYER \n",
      " 0.11564340  0.23128680  0.11564340  0.11564340  0.11564340  0.11564340 \n",
      "        DMD         AND         JIM     CLYBURN         DSC   SIMILARLY \n",
      " 0.11564340  0.03093614  0.11564340  0.11564340  0.11564340  0.11564340 \n",
      "    STEPPED        DOWN        FROM       THEIR  LEADERSHIP       ROLES \n",
      " 0.11564340  0.11564340  0.07916197  0.11564340  0.11564340  0.11564340 \n",
      "         SPEAKER          EMERITA           PELOSI               IS \n",
      "      0.10027183       0.14648164       0.14648164       0.03918578 \n",
      "             ONE               OF              THE             MOST \n",
      "      0.10027183       0.07837156       0.01675430       0.14648164 \n",
      "        TALENTED              AND TRANSFORMATIONAL          LEADERS \n",
      "      0.14648164       0.03918578       0.14648164       0.14648164 \n",
      "             OUR         LIFETIME \n",
      "      0.14648164       0.14648164 \n"
     ]
    }
   ],
   "source": [
    "#2 Testing\n",
    "for (i in tfidf_pipeline(corpus)) {\n",
    "  print(i)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp6jTWfCCxV_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA8CHwGQJkGq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
